{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vegan111-vegan11/rag/blob/main/%EC%9D%B4%EC%A0%84_%EB%8C%80%ED%99%94%EB%A5%BC_%EA%B8%B0%EC%96%B5%ED%95%98%EB%8A%94_Langchain_%EC%82%AC%EC%9A%A9%EB%B2%95_4%EA%B0%80%EC%A7%80_%7C_%EB%81%9D%EB%A7%90%EC%9E%87%EA%B8%B0_%EC%B1%97%EB%B4%87_%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai\n",
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ttQhY7GjOOji",
        "outputId": "d69a614c-ba0f-4b4e-b0b6-c402b3a1bd6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.9 (from langchain_openai)\n",
            "  Downloading langchain_core-0.3.9-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain_openai)\n",
            "  Downloading openai-1.51.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.9->langchain_openai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-core<0.4.0,>=0.3.9->langchain_openai)\n",
            "  Downloading langsmith-0.1.132-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (2.9.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.4.0,>=0.3.9->langchain_openai)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading jiter-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.9->langchain_openai)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.9->langchain_openai)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.9->langchain_openai)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.9->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.9->langchain_openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.3)\n",
            "Downloading langchain_openai-0.2.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.9-py3-none-any.whl (401 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m401.8/401.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.51.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m326.6/326.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.132-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, jiter, h11, tiktoken, requests-toolbelt, jsonpatch, httpcore, httpx, openai, langsmith, langchain-core, langchain_openai\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.9 langchain_openai-0.2.2 langsmith-0.1.132 openai-1.51.1 orjson-3.10.7 requests-toolbelt-1.0.0 tenacity-8.5.0 tiktoken-0.8.0\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.8)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.1 (from langchain_community)\n",
            "  Downloading langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.9)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.132)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.1->langchain_community)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.1->langchain_community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain_community) (2.23.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.2)\n",
            "Downloading langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain-0.3.2-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain-text-splitters, langchain, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-0.3.2 langchain-text-splitters-0.3.0 langchain_community-0.3.1 marshmallow-3.22.0 mypy-extensions-1.0.0 pydantic-settings-2.5.2 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
      ],
      "metadata": {
        "id": "dTxPYf8IRefE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. í™˜ê²½ì„¤ì •"
      ],
      "metadata": {
        "id": "YJM11xQ_qWWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Langchain ì œê³µ ê¸°ëŠ¥"
      ],
      "metadata": {
        "id": "vyTg2PGOqbsi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì°¸ê³  ìë£Œ : https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/"
      ],
      "metadata": {
        "id": "AItlxxNmRLeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Message passing\n",
        "- ì´ì „ ëŒ€í™” ë‚´ìš©ì„ chainì— ë„˜ê²¨ì£¼ëŠ” ê²ƒ\n",
        "\n",
        "- The simplest form of memory is simply passing chat history messages into a chain."
      ],
      "metadata": {
        "id": "N5XYv0azqf3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model=\"gpt-4o\") # gpt-3.5-turbo-0125"
      ],
      "metadata": {
        "id": "bnr2xJAmRM5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability. YOU MUST ANSWER IN KOREAN.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat\n",
        "\n",
        "ai_msg = chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            (\n",
        "                \"human\",\n",
        "                \"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\",\n",
        "            ),\n",
        "            (\"ai\",  \"ã„ãŸã ãã¾ã™ (ì´íƒ€ë‹¤í‚¤ë§ˆìŠ¤)\"),\n",
        "            (\"human\", \"ë‚´ê°€ ë°©ê¸ˆ ë­ë¼ê³  í–ˆì§€?\"),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "print(ai_msg.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mphlBET1RY8U",
        "outputId": "db12cc1f-a5b4-4465-9989-1a369a64d11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¹ì‹ ì€ ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.'ë¼ëŠ” í‘œí˜„ì„ ë¬¼ì–´ë³´ì…¨ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ì— ëŒ€í•œ ë‹µìœ¼ë¡œ ì¼ë³¸ì–´ë¡œëŠ” \"ã„ãŸã ãã¾ã™\" (ì´íƒ€ë‹¤í‚¤ë§ˆìŠ¤)ë¼ê³  ë§í•œë‹¤ê³  ì•Œë ¤ë“œë ¸ìŠµë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_msg = chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            (\n",
        "                \"human\",\n",
        "                \"ë‚´ ì´ë¦„ì€ ê³µì›ë‚˜ì—°ì´ê³ , AIë¥¼ ì•Œë ¤ì£¼ëŠ” ìœ íŠœë¸Œë¥¼ ìš´ì˜í•˜ê³  ìˆì–´.\",\n",
        "            ),\n",
        "            (\"ai\",  \"ê·¸ë ‡êµ°ìš”. í¥ë¯¸ë¡œìš´ë°ìš”?\"),\n",
        "            (\"human\", \"ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?\"),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "print(ai_msg.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-nxFkBPTnL_",
        "outputId": "06c9ef97-8fa3-4d78-a96b-41e219124fcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë‹¹ì‹ ì˜ ì´ë¦„ì€ ê³µì›ë‚˜ì—°ì…ë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ì‘ìš© : ëŒ€í™” ìŒ“ì¼ ë•Œë§ˆë‹¤ íˆìŠ¤í† ë¦¬ ì €ì¥í•˜ë„ë¡ êµ¬í˜„"
      ],
      "metadata": {
        "id": "uzZlHV2vMP5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_list = []\n",
        "while(True):\n",
        "    user_input = input()\n",
        "    if user_input == \"ì¢…ë£Œ\": break\n",
        "    history_list.append(\n",
        "        (\n",
        "            \"human\",\n",
        "            user_input,\n",
        "        )\n",
        "    )\n",
        "    print(\"## CHAT_HISTORY ##\")\n",
        "    print(history_list, \"\\n\")\n",
        "    ai_msg = chain.invoke(\n",
        "        {\n",
        "            \"messages\": history_list,\n",
        "        }\n",
        "    )\n",
        "    print(\"AI Says : \",ai_msg.content)\n",
        "\n",
        "    history_list.append(\n",
        "        (\n",
        "            \"ai\",\n",
        "            ai_msg.content,\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnGLhdn1LxJI",
        "outputId": "40353192-7f77-4132-f6cb-74fda196ff58"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì•ˆë…•\n",
            "## CHAT_HISTORY ##\n",
            "[('human', 'ì•ˆë…•')] \n",
            "\n",
            "AI Says :  ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
            "ë‚œ ë‚˜ì—°ì´ë¼ê³  í•´.\n",
            "## CHAT_HISTORY ##\n",
            "[('human', 'ì•ˆë…•'), ('ai', 'ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?'), ('human', 'ë‚œ ë‚˜ì—°ì´ë¼ê³  í•´.')] \n",
            "\n",
            "AI Says :  ì•ˆë…•í•˜ì„¸ìš”, ë‚˜ì—°ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
            "ê·¸ë¦¬ê³  ë§ì°¨ë¥¼ ì¢‹ì•„í•˜ì§€\n",
            "## CHAT_HISTORY ##\n",
            "[('human', 'ì•ˆë…•'), ('ai', 'ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?'), ('human', 'ë‚œ ë‚˜ì—°ì´ë¼ê³  í•´.'), ('ai', 'ì•ˆë…•í•˜ì„¸ìš”, ë‚˜ì—°ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?'), ('human', 'ê·¸ë¦¬ê³  ë§ì°¨ë¥¼ ì¢‹ì•„í•˜ì§€')] \n",
            "\n",
            "AI Says :  ë§ì°¨ë¥¼ ì¢‹ì•„í•˜ì‹œëŠ”êµ°ìš”! ë§ì°¨ëŠ” ë…íŠ¹í•œ ë§›ê³¼ í–¥ì´ ìˆì–´ì„œ ë§ì€ ì‚¬ëŒë“¤ì´ ì¢‹ì•„í•˜ì£ . ë§ì°¨ë¥¼ ìì£¼ ë“œì‹œë‚˜ìš”, ì•„ë‹ˆë©´ íŠ¹ë³„í•œ ë•Œì—ë§Œ ë“œì‹œë‚˜ìš”?\n",
            "ë‚´ ì´ë¦„ì´ ë­ì˜€ê²Œ?\n",
            "## CHAT_HISTORY ##\n",
            "[('human', 'ì•ˆë…•'), ('ai', 'ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?'), ('human', 'ë‚œ ë‚˜ì—°ì´ë¼ê³  í•´.'), ('ai', 'ì•ˆë…•í•˜ì„¸ìš”, ë‚˜ì—°ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?'), ('human', 'ê·¸ë¦¬ê³  ë§ì°¨ë¥¼ ì¢‹ì•„í•˜ì§€'), ('ai', 'ë§ì°¨ë¥¼ ì¢‹ì•„í•˜ì‹œëŠ”êµ°ìš”! ë§ì°¨ëŠ” ë…íŠ¹í•œ ë§›ê³¼ í–¥ì´ ìˆì–´ì„œ ë§ì€ ì‚¬ëŒë“¤ì´ ì¢‹ì•„í•˜ì£ . ë§ì°¨ë¥¼ ìì£¼ ë“œì‹œë‚˜ìš”, ì•„ë‹ˆë©´ íŠ¹ë³„í•œ ë•Œì—ë§Œ ë“œì‹œë‚˜ìš”?'), ('human', 'ë‚´ ì´ë¦„ì´ ë­ì˜€ê²Œ?')] \n",
            "\n",
            "AI Says :  ë‚˜ì—°ë‹˜ì´ì‹œì£ ? ì•ì—ì„œ ë§ì”€í•´ ì£¼ì…¨ìŠµë‹ˆë‹¤. ğŸ˜Š\n",
            "êµ¿\n",
            "## CHAT_HISTORY ##\n",
            "[('human', 'ì•ˆë…•'), ('ai', 'ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?'), ('human', 'ë‚œ ë‚˜ì—°ì´ë¼ê³  í•´.'), ('ai', 'ì•ˆë…•í•˜ì„¸ìš”, ë‚˜ì—°ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?'), ('human', 'ê·¸ë¦¬ê³  ë§ì°¨ë¥¼ ì¢‹ì•„í•˜ì§€'), ('ai', 'ë§ì°¨ë¥¼ ì¢‹ì•„í•˜ì‹œëŠ”êµ°ìš”! ë§ì°¨ëŠ” ë…íŠ¹í•œ ë§›ê³¼ í–¥ì´ ìˆì–´ì„œ ë§ì€ ì‚¬ëŒë“¤ì´ ì¢‹ì•„í•˜ì£ . ë§ì°¨ë¥¼ ìì£¼ ë“œì‹œë‚˜ìš”, ì•„ë‹ˆë©´ íŠ¹ë³„í•œ ë•Œì—ë§Œ ë“œì‹œë‚˜ìš”?'), ('human', 'ë‚´ ì´ë¦„ì´ ë­ì˜€ê²Œ?'), ('ai', 'ë‚˜ì—°ë‹˜ì´ì‹œì£ ? ì•ì—ì„œ ë§ì”€í•´ ì£¼ì…¨ìŠµë‹ˆë‹¤. ğŸ˜Š'), ('human', 'êµ¿')] \n",
            "\n",
            "AI Says :  ê°ì‚¬í•©ë‹ˆë‹¤! ë‚˜ì—°ë‹˜, ë‹¤ë¥¸ ê¶ê¸ˆí•œ ì ì´ë‚˜ ì´ì•¼ê¸°ê°€ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.\n",
            "ì¢…ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Chat history\n",
        "- ChatMessageHistory() í´ë˜ìŠ¤ ì‚¬ìš©í•˜ê¸°\n",
        "- It's perfectly fine to store and pass messages directly as an array, but we can use LangChain's built-in message history class to store and load messages as well."
      ],
      "metadata": {
        "id": "lHsjha-JTVMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://api.python.langchain.com/en/latest/chat_message_histories/langchain_community.chat_message_histories.in_memory.ChatMessageHistory.html"
      ],
      "metadata": {
        "id": "4EZYo9IoXgMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "chat_history.add_user_message(\n",
        "    \"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\"\n",
        ")\n",
        "\n",
        "chat_history.add_ai_message(\"ã„ãŸã ãã¾ã™ (ì´íƒ€ë‹¤í‚¤ë§ˆìŠ¤)\")\n",
        "\n",
        "chat_history.messages\n",
        "# chat_history.clear()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82OuUOtETIpI",
        "outputId": "1d75e123-2254-46bf-966c-b01481241bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='ã„ãŸã ãã¾ã™ (ì´íƒ€ë‹¤í‚¤ë§ˆìŠ¤)', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ëŒ€í™”ë‚´ìš© ë°”ë¡œë°”ë¡œ ì €ì¥í•˜ê¸°"
      ],
      "metadata": {
        "id": "dLOjjo9sWKdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "################ USER INPUT - 1 ################\n",
        "input1 = \"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\"\n",
        "\n",
        "# ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì…ë ¥(user_input) ì €ì¥\n",
        "chat_history.add_user_message(input1)\n",
        "print(\"ì²«ë²ˆì§¸ ì…ë ¥ í›„ history : \", chat_history.messages)\n",
        "\n",
        "################ AI RESPONSE - 1 ################\n",
        "# ë‹µë³€ ìƒì„±(response)\n",
        "response = chain.invoke(\n",
        "    {\n",
        "        \"messages\": chat_history.messages,\n",
        "    }\n",
        ")\n",
        "\n",
        "# ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ë‹µë³€(response) ì €ì¥\n",
        "chat_history.add_ai_message(response)\n",
        "print(\"ì²«ë²ˆì§¸ ë‹µë³€ í›„ history : \", chat_history.messages)\n",
        "print()\n",
        "\n",
        "################ USER INPUT - 2 ################\n",
        "input2 = \"ë‚´ê°€ ë°©ê¸ˆ ë­ë¼ê³  ë¬¼ì–´ë´¤ì§€?\"\n",
        "\n",
        "# ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì…ë ¥(user_input) ì €ì¥\n",
        "chat_history.add_user_message(input2)\n",
        "print(\"ë‘ë²ˆì§¸ ì…ë ¥ í›„ history : \", chat_history.messages)\n",
        "\n",
        "################ AI RESPONSE - 2 ################\n",
        "# ë‹µë³€ ìƒì„±(response)\n",
        "print(\"[ë‘ë²ˆì§¸ ì…ë ¥ì— ëŒ€í•œ ë‹µë³€]\")\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"messages\": chat_history.messages,\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0msG3hgTVvF",
        "outputId": "6818c7dd-a98a-4581-c9ff-eab36c4c1c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì²«ë²ˆì§¸ ì…ë ¥ í›„ history :  [HumanMessage(content=\"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\", additional_kwargs={}, response_metadata={})]\n",
            "ì²«ë²ˆì§¸ ë‹µë³€ í›„ history :  [HumanMessage(content=\"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\", additional_kwargs={}, response_metadata={}), AIMessage(content='ì¼ë³¸ì–´ë¡œ \\'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.\\'ëŠ” \"ã„ãŸã ãã¾ã™\"ë¼ê³  ë§í•©ë‹ˆë‹¤. í•œê¸€ ë°œìŒì€ \"ì´íƒ€ë‹¤í‚¤ë§ˆìŠ¤\"ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 59, 'total_tokens': 93, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2f406b9113', 'finish_reason': 'stop', 'logprobs': None}, id='run-1ed0525d-6e28-4e28-9c7a-a8970c5b9c49-0', usage_metadata={'input_tokens': 59, 'output_tokens': 34, 'total_tokens': 93, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]\n",
            "\n",
            "ë‘ë²ˆì§¸ ì…ë ¥ í›„ history :  [HumanMessage(content=\"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\", additional_kwargs={}, response_metadata={}), AIMessage(content='ì¼ë³¸ì–´ë¡œ \\'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.\\'ëŠ” \"ã„ãŸã ãã¾ã™\"ë¼ê³  ë§í•©ë‹ˆë‹¤. í•œê¸€ ë°œìŒì€ \"ì´íƒ€ë‹¤í‚¤ë§ˆìŠ¤\"ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 59, 'total_tokens': 93, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2f406b9113', 'finish_reason': 'stop', 'logprobs': None}, id='run-1ed0525d-6e28-4e28-9c7a-a8970c5b9c49-0', usage_metadata={'input_tokens': 59, 'output_tokens': 34, 'total_tokens': 93, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content='ë‚´ê°€ ë°©ê¸ˆ ë­ë¼ê³  ë¬¼ì–´ë´¤ì§€?', additional_kwargs={}, response_metadata={})]\n",
            "[ë‘ë²ˆì§¸ ì…ë ¥ì— ëŒ€í•œ ë‹µë³€]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='ë‹¹ì‹ ì€ \"ì¼ë³¸ì–´ë¡œ \\'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.\\' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\"ë¼ê³  ë¬¼ì–´ë³´ì…¨ìŠµë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 112, 'total_tokens': 146, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_e5e4913e83', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a35d5c0-d930-4eef-a5fa-11cf78860163-0', usage_metadata={'input_tokens': 112, 'output_tokens': 34, 'total_tokens': 146, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ì‘ìš© : ëŒ€í™” ìŒ“ì¼ ë•Œë§ˆë‹¤ íˆìŠ¤í† ë¦¬ ì €ì¥í•˜ë„ë¡ êµ¬í˜„"
      ],
      "metadata": {
        "id": "8lNztnwgY5G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "while(True):\n",
        "    user_input = input()\n",
        "    if user_input == \"ì¢…ë£Œ\": break\n",
        "    chat_history.add_user_message(user_input)\n",
        "    response = chain.invoke(\n",
        "        {\n",
        "            \"messages\": chat_history.messages,\n",
        "        }\n",
        "    )\n",
        "    chat_history.add_ai_message(response)\n",
        "    print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atmi65POYM12",
        "outputId": "5842e088-b8d0-4b7b-bdde-8bbeca9aeefd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì•ˆë…•!\n",
            "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
            "ë§ì°¨ë¡œ ë§Œë“  ë””ì €íŠ¸ì—ëŠ” ë­ê°€ ìˆì–´?\n",
            "ë§ì°¨ë¡œ ë§Œë“  ë””ì €íŠ¸ì—ëŠ” ë‹¤ì–‘í•œ ì¢…ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤. ëª‡ ê°€ì§€ ì˜ˆë¥¼ ë“¤ë©´:\n",
            "\n",
            "1. **ë§ì°¨ í‹°ë¼ë¯¸ìˆ˜** - ì „í†µì ì¸ í‹°ë¼ë¯¸ìˆ˜ì— ë§ì°¨ë¥¼ ë”í•´ ìƒˆë¡œìš´ ë§›ì„ ì¦ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "2. **ë§ì°¨ ë§ˆì¹´ë¡±** - ê²‰ì€ ë°”ì‚­í•˜ê³  ì†ì€ ë¶€ë“œëŸ¬ìš´ ë§ˆì¹´ë¡±ì— ë§ì°¨ í¬ë¦¼ì„ ì±„ì›Œ ë„£ì€ ë””ì €íŠ¸ì…ë‹ˆë‹¤.\n",
            "3. **ë§ì°¨ ì•„ì´ìŠ¤í¬ë¦¼** - ë¶€ë“œëŸ½ê³  ì§„í•œ ë§ì°¨ ë§›ì´ ëŠê»´ì§€ëŠ” ì•„ì´ìŠ¤í¬ë¦¼ì…ë‹ˆë‹¤.\n",
            "4. **ë§ì°¨ ë¡¤ì¼€ì´í¬** - ë¶€ë“œëŸ¬ìš´ ìŠ¤í°ì§€ ì¼€ì´í¬ì— ë§ì°¨ í¬ë¦¼ì„ ë°œë¼ ë§ì•„ì„œ ë§Œë“  ì¼€ì´í¬ì…ë‹ˆë‹¤.\n",
            "5. **ë§ì°¨ ë¸Œë¼ìš°ë‹ˆ** - ì§„í•œ ì´ˆì½œë¦¿ ë¸Œë¼ìš°ë‹ˆì— ë§ì°¨ë¥¼ ë”í•´ ìƒˆë¡œìš´ í’ë¯¸ë¥¼ ë”í•œ ë””ì €íŠ¸ì…ë‹ˆë‹¤.\n",
            "6. **ë§ì°¨ í‘¸ë”©** - ë¶€ë“œëŸ½ê³  í¬ë¦¬ë¯¸í•œ ë§ì°¨ ë§›ì˜ í‘¸ë”©ì…ë‹ˆë‹¤.\n",
            "7. **ë§ì°¨ íŒŒìš´ë“œ ì¼€ì´í¬** - ì´‰ì´‰í•˜ê³  ì§„í•œ ë§ì°¨ ë§›ì´ ëŠê»´ì§€ëŠ” íŒŒìš´ë“œ ì¼€ì´í¬ì…ë‹ˆë‹¤.\n",
            "\n",
            "ì´ ì™¸ì—ë„ ë‹¤ì–‘í•œ ì°½ì˜ì ì¸ ë§ì°¨ ë””ì €íŠ¸ë“¤ì´ ìˆìœ¼ë‹ˆ ì‹œë„í•´ë³´ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤!\n",
            "ì´ ì¤‘ì—ì„œ ê°€ì¥ ë§Œë“¤ê¸° ì‰¬ìš´ê±´ ë­˜ê¹Œ?\n",
            "ë§ì°¨ ë””ì €íŠ¸ ì¤‘ì—ì„œ ë¹„êµì  ë§Œë“¤ê¸° ì‰¬ìš´ ê²ƒì€ **ë§ì°¨ ì•„ì´ìŠ¤í¬ë¦¼**ì´ë‚˜ **ë§ì°¨ í‘¸ë”©**ì…ë‹ˆë‹¤. \n",
            "\n",
            "- **ë§ì°¨ ì•„ì´ìŠ¤í¬ë¦¼**: ì•„ì´ìŠ¤í¬ë¦¼ ì œì¡°ê¸°ì— í•„ìš”í•œ ì¬ë£Œë¥¼ ì„ì–´ì„œ ì–¼ë¦¬ê¸°ë§Œ í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— ë¹„êµì  ê°„ë‹¨í•©ë‹ˆë‹¤. ë§Œì•½ ì•„ì´ìŠ¤í¬ë¦¼ ì œì¡°ê¸°ê°€ ì—†ë‹¤ë©´, ìƒí¬ë¦¼ê³¼ ì—°ìœ , ë§ì°¨ ê°€ë£¨ë¥¼ ì„ì–´ ëƒ‰ë™ì‹¤ì— ì–¼ë¦¬ë©´ì„œ ì¤‘ê°„ì¤‘ê°„ ì “ëŠ” ë°©ë²•ìœ¼ë¡œë„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "- **ë§ì°¨ í‘¸ë”©**: ì ¤ë¼í‹´ì„ ì‚¬ìš©í•˜ì—¬ í‘¸ë”©ì„ êµ³íˆëŠ” ë°©ì‹ì´ë¼ ë³µì¡í•œ ë² ì´í‚¹ ê¸°ìˆ ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìš°ìœ , ì„¤íƒ•, ë§ì°¨ ê°€ë£¨, ì ¤ë¼í‹´ì„ ì˜ ì„ì–´ì„œ êµ³íˆê¸°ë§Œ í•˜ë©´ ë˜ë¯€ë¡œ ì´ˆë³´ìë„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "ì´ ë‘ ê°€ì§€ëŠ” ë¹„êµì  ê°„ë‹¨í•œ ì¬ë£Œì™€ ê³¼ì •ì„ í†µí•´ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë‹ˆ ì²˜ìŒ ì‹œë„í•´ë³´ê¸° ì¢‹ìŠµë‹ˆë‹¤.\n",
            "ê·¸ëŸ¼ ë§ì°¨ ë¸Œë¼ìš°ë‹ˆëŠ”?\n",
            "ë§ì°¨ ë¸Œë¼ìš°ë‹ˆëŠ” ê¸°ë³¸ì ì¸ ë² ì´í‚¹ ê¸°ìˆ ì´ í•„ìš”í•˜ì§€ë§Œ, ë¹„êµì  ê°„ë‹¨í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ë””ì €íŠ¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì—¬ê¸° ê°„ë‹¨í•œ ë§ì°¨ ë¸Œë¼ìš°ë‹ˆ ë ˆì‹œí”¼ë¥¼ ì†Œê°œí•´ë“œë¦´ê²Œìš”.\n",
            "\n",
            "**ì¬ë£Œ:**\n",
            "- ë°€ê°€ë£¨ 100g\n",
            "- ì„¤íƒ• 150g\n",
            "- ë²„í„° 100g\n",
            "- ë‹¬ê±€ 2ê°œ\n",
            "- ë§ì°¨ ê°€ë£¨ 2í°ìˆ \n",
            "- ë² ì´í‚¹ íŒŒìš°ë” 1ì‘ì€ìˆ \n",
            "- ì†Œê¸ˆ ì•½ê°„\n",
            "- í™”ì´íŠ¸ ì´ˆì½œë¦¿ ì¹© (ì„ íƒì‚¬í•­)\n",
            "\n",
            "**ë§Œë“œëŠ” ë²•:**\n",
            "1. ì˜¤ë¸ì„ 180ë„ë¡œ ì˜ˆì—´í•©ë‹ˆë‹¤.\n",
            "2. ë²„í„°ëŠ” ë…¹ì—¬ì„œ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
            "3. í° ë³¼ì— ë…¹ì¸ ë²„í„°ì™€ ì„¤íƒ•ì„ ë„£ê³  ì˜ ì„ì–´ì¤ë‹ˆë‹¤.\n",
            "4. ë‹¬ê±€ì„ í•˜ë‚˜ì”© ë„£ê³  ê±°í’ˆê¸°ë¡œ ì˜ ì €ì–´ì¤ë‹ˆë‹¤.\n",
            "5. ì²´ë¡œ ì¹œ ë°€ê°€ë£¨, ë§ì°¨ ê°€ë£¨, ë² ì´í‚¹ íŒŒìš°ë”, ì†Œê¸ˆì„ ë„£ê³  ì£¼ê±±ìœ¼ë¡œ ì˜ ì„ì–´ì¤ë‹ˆë‹¤. \n",
            "6. ì„ íƒì‚¬í•­ìœ¼ë¡œ í™”ì´íŠ¸ ì´ˆì½œë¦¿ ì¹©ì„ ë„£ê³  ì„ì–´ì¤ë‹ˆë‹¤.\n",
            "7. ë°˜ì£½ì„ ë² ì´í‚¹ í‹€ì— ë¶€ì–´ ê³ ë¥´ê²Œ í¼ì¹©ë‹ˆë‹¤.\n",
            "8. ì˜ˆì—´ëœ ì˜¤ë¸ì—ì„œ ì•½ 20-25ë¶„ê°„ êµ¬ì›Œì¤ë‹ˆë‹¤. ê¼¬ì¹˜ë¡œ ì°”ëŸ¬ë³´ì•„ ë°˜ì£½ì´ ë¬»ì–´ë‚˜ì˜¤ì§€ ì•Šìœ¼ë©´ ì™„ì„±ì…ë‹ˆë‹¤.\n",
            "9. ì˜¤ë¸ì—ì„œ êº¼ë‚´ì–´ ì‹íŒ í›„, ì ë‹¹í•œ í¬ê¸°ë¡œ ì˜ë¼ì„œ ì¦ê¸°ì‹œë©´ ë©ë‹ˆë‹¤.\n",
            "\n",
            "ì´ ë ˆì‹œí”¼ë¥¼ ë”°ë¼ í•˜ë©´ ì§‘ì—ì„œë„ ë§›ìˆëŠ” ë§ì°¨ ë¸Œë¼ìš°ë‹ˆë¥¼ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "ì¢…ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Automatic history management\n",
        "- RunnableWithMessageHistory() í™œìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ê´€ë¦¬í•˜ê¸°\n",
        "- The previous examples pass messages to the chain explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also includes an wrapper for LCEL chains that can handle this process automatically called RunnableWithMessageHistory."
      ],
      "metadata": {
        "id": "h5IW96A8WNYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(model=\"gpt-4o\")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability. YOU MUST ANSWER IN KOREAN.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat"
      ],
      "metadata": {
        "id": "ECXuHzfWWNol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "chat_history_for_chain = ChatMessageHistory()\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    chain, # ì‹¤í–‰í•  Runnable ê°ì²´\n",
        "    lambda session_id: chat_history_for_chain, # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
        "    input_messages_key=\"input\", # ì…ë ¥ ë©”ì‹œì§€ì˜ Key\n",
        "    history_messages_key=\"chat_history\", # ëŒ€í™” íˆìŠ¤í† ë¦¬ ë©”ì‹œì§€ì˜ Key\n",
        ")"
      ],
      "metadata": {
        "id": "y2GV7xqWWPGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0QE2_qxWPkl",
        "outputId": "2a7fd8b4-bb9e-49e6-b992-5f9445971d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='ì¼ë³¸ì–´ë¡œ \\'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.\\'ëŠ” \"ã„ãŸã ãã¾ã™\"ë¼ê³  ë§í•©ë‹ˆë‹¤. í•œê¸€ ë°œìŒì€ \"ì´íƒ€ë‹¤í‚¤ë§ˆìŠ¤\"ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 59, 'total_tokens': 93, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_e5e4913e83', 'finish_reason': 'stop', 'logprobs': None}, id='run-6ea25a7f-eaa5-4809-81eb-57d41ad37427-0', usage_metadata={'input_tokens': 59, 'output_tokens': 34, 'total_tokens': 93, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"ë‚´ê°€ ë­ë¼ê³  ë¬¼ì–´ë´¤ì§€?\"}, {\"configurable\": {\"session_id\": \"unused\"}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5898B2zWRQT",
        "outputId": "8a0777ca-7493-4564-fe0f-ff730ad68a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='ë‹¹ì‹ ì€ \"\\'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.\\'ë¥¼ ì¼ë³¸ì–´ë¡œ ì–´ë–»ê²Œ ë§í•˜ëŠ”ì§€\" ë¬¼ì–´ë³´ì…¨ìŠµë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 110, 'total_tokens': 133, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_e5e4913e83', 'finish_reason': 'stop', 'logprobs': None}, id='run-50037728-4fa6-4506-ba54-a5d97a92e832-0', usage_metadata={'input_tokens': 110, 'output_tokens': 23, 'total_tokens': 133, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "íˆìŠ¤í† ë¦¬ ì €ì¥ í•¨ìˆ˜ë¥¼ ë”°ë¡œ í˜¸ì¶œí•  í•„ìš”ì—†ì´ invoke ì‹¤í–‰ë§Œìœ¼ë¡œ ëŒ€í™” ê¸°ë¡ì´ ìë™ ì €ì¥ë˜ëŠ” ê²ƒì„ í™•ì¸ ê°€ëŠ¥!"
      ],
      "metadata": {
        "id": "mwFQUG_PgvGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Modifying chat history\n",
        "- ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê°€ê³µí•˜ì—¬ ë„˜ê²¨ì£¼ê¸°\n",
        "- Modifying stored chat messages can help your chatbot handle a variety of situations."
      ],
      "metadata": {
        "id": "R9HtRAXzfutg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "chat_history.add_user_message(\"ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ ë‚˜ì—°ì…ë‹ˆë‹¤.\")\n",
        "chat_history.add_ai_message(\"ì•ˆë…•í•˜ì„¸ìš”, ë‚˜ì—°ë‹˜! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\")\n",
        "chat_history.add_user_message(\"ë‚ ì”¨ ì¢‹ì€ ë‚  ë“¤ì„ë§Œ í•œ ë…¸ë˜ ì¶”ì²œí•´ì£¼ì„¸ìš”.\")\n",
        "chat_history.add_ai_message(\"Carpenters - Close to you ë¥¼ ì¶”ì²œí•´ìš”.\")\n",
        "\n",
        "chat_history.messages\n",
        "# chat_history.clear()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJicGgSXWvkL",
        "outputId": "9b8deca3-bf20-49d8-e38f-049c1d22bd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ ë‚˜ì—°ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, ë‚˜ì—°ë‹˜! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='ë‚ ì”¨ ì¢‹ì€ ë‚  ë“¤ì„ë§Œ í•œ ë…¸ë˜ ì¶”ì²œí•´ì£¼ì„¸ìš”.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Carpenters - Close to you ë¥¼ ì¶”ì²œí•´ìš”.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability. The provided chat history includes facts about the user you are speaking with. YOU MUST ANSWER IN KOREAN.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: chat_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")"
      ],
      "metadata": {
        "id": "1f7vAPaCWyyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "def summarize_messages(chain_input):\n",
        "    stored_messages = chat_history.messages\n",
        "    if len(stored_messages) == 0:\n",
        "        return False\n",
        "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"placeholder\", \"{chat_history}\"),\n",
        "            (\n",
        "                \"user\",\n",
        "                \"Distill the above chat messages into a single summary message. Include as many specific details as you can.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    summarization_chain = summarization_prompt | chat\n",
        "\n",
        "    # chat_history ì— ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ì„ ìš”ì•½í”„ë¡¬í”„íŠ¸ì— ì…ë ¥ & ê²°ê³¼ ì €ì¥\n",
        "    summary_message = summarization_chain.invoke({\"chat_history\": stored_messages})\n",
        "\n",
        "    # chat_history ì— ì €ì¥ë˜ì–´ìˆë˜ ê¸°ë¡ ì§€ìš°ê¸°\n",
        "    chat_history.clear()\n",
        "\n",
        "    # ìƒì„±ëœ ìƒˆë¡œìš´ ìš”ì•½ë‚´ìš©ìœ¼ë¡œ ê¸°ë¡ ì±„ìš°ê¸°\n",
        "    chat_history.add_message(summary_message)\n",
        "\n",
        "    return True\n",
        "\n",
        "chain_with_summarization = (\n",
        "    RunnablePassthrough.assign(messages_summarized=summarize_messages)\n",
        "    | chain_with_message_history\n",
        ")"
      ],
      "metadata": {
        "id": "NEvcOsdqWzsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- summarize_messages í•¨ìˆ˜ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ messages_summarized ë¼ëŠ” ì´ë¦„ì˜ Key ë¡œ ì €ì¥\n",
        "\n",
        "    - ì •ì˜í•œ chain ì•ˆì— \"messages_summarized\" Key ê°€ ì •ì˜ë˜ê³  ìˆì§€ ì•Šìœ¼ë¯€ë¡œ, ì´ëŠ” `summarize_messages` í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ë¬´ì‹œ ê°€ëŠ¥í•œ Key\n",
        "    - í•¨ìˆ˜ê°€ ì‹¤í–‰ë˜ë©´ chat_history ì— ìš”ì•½ëœ íˆìŠ¤í† ë¦¬ê°€ ì €ì¥ë¨\n"
      ],
      "metadata": {
        "id": "3Ed_Kcek8kvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_summarization.invoke(\n",
        "    {\"input\": \"ì œ ì´ë¦„ì„ ê¸°ì–µí•˜ê³  ìˆë‚˜ìš”?\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq4jhbBBW17O",
        "outputId": "a2a1c7ba-3052-484f-ad32-cf00b70e69ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='ë„¤, ë‚˜ì—°ë‹˜ì´ë¼ê³  ì•Œê³  ìˆìŠµë‹ˆë‹¤. ë§ë‚˜ìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 94, 'total_tokens': 106, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2f406b9113', 'finish_reason': 'stop', 'logprobs': None}, id='run-0e55a4c3-cc75-4b51-afc6-41fc57f30d60-0', usage_metadata={'input_tokens': 94, 'output_tokens': 12, 'total_tokens': 106, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_summarization.invoke(\n",
        "    {\"input\": \"ê·¸ ê°€ìˆ˜ëŠ” ë‚¨ìì¸ê°€ìš” ì—¬ìì¸ê°€ìš”?\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ],
      "metadata": {
        "id": "XtwtUrBcW6eK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a2913b-265e-4ce4-a1c3-d38588178034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='CarpentersëŠ” ë‚¨ë§¤ ë“€ì˜¤ë¡œ, ë¦¬ì²˜ë“œ ì¹´íœí„°ì™€ ì¹´ë Œ ì¹´íœí„°ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì¹´ë Œ ì¹´íœí„°ëŠ” ì—¬ì„± ë³´ì»¬ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 109, 'total_tokens': 153, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_e5e4913e83', 'finish_reason': 'stop', 'logprobs': None}, id='run-20fa421b-37ee-49f2-8af2-dbc6ea408fd6-0', usage_metadata={'input_tokens': 109, 'output_tokens': 44, 'total_tokens': 153, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C-goCIryhxk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. ëë§ì‡ê¸° ê²Œì„ êµ¬í˜„í•˜ê¸°\n",
        "\n",
        "- 4) Modifying chat history ì˜ í™œìš©!"
      ],
      "metadata": {
        "id": "Rn0LO7Bdr8nT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  íˆìŠ¤í† ë¦¬ í´ë˜ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "# chat_history.add_user_message(\"ëë§ì‡ê¸° í•˜ì\")\n",
        "# chat_history.add_ai_message(\"ì¢‹ìŠµë‹ˆë‹¤. ì œê°€ ë¨¼ì € ì‹œì‘í• ê²Œìš”. ë°”ë‚˜ë‚˜!\")\n",
        "# chat_history.add_user_message(\"ë‚˜ì´í…Œ\")\n",
        "# chat_history.add_ai_message(\"í…Œì´í”„\")\n",
        "\n",
        "chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ed50a9-b5bb-458c-db66-6c2550074ded",
        "id": "OpnDi8ZFhx6y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(model=\"gpt-4o\")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"ë‹¹ì‹ ì€ ëë§ì‡ê¸° ê²Œì„ì„ ì§„í–‰í•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ê²Œì„ ê·œì¹™ì…ë‹ˆë‹¤. ë‹¹ì‹ ê³¼ user ì˜ ì…ë ¥ì—ì„œ ì•„ë˜ ê·œì¹™ì´ ê¼­ ì§€ì¼œì ¸ì•¼ í•˜ë©°, ì§€í‚¤ì§€ ì•Šì€ ì‚¬ëŒì—ê²Œ íŒ¨ë°°ë¥¼ ì•Œë¦° ë’¤, ëë§ì‡ê¸° ê²Œì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n",
        "                1. ì£¼ì–´ì§„ ëŒ€í™” ê¸°ë¡ì—ì„œ ì´ë¯¸ ë‚˜ì™”ë˜ ë‹¨ì–´ë¥¼ ë‹¤ì‹œ ë§í–ˆì„ ê²½ìš° íŒ¨ë°°í•©ë‹ˆë‹¤.\n",
        "                2. ë‘ìŒë²•ì¹™ì„ í—ˆìš©í•©ë‹ˆë‹¤. (ex. ë¦¬ -> ì´, ë ¥ -> ì—­, ë½ -> ë‚™)\n",
        "                3. êµ­ì–´ì‚¬ì „ì— ì¡´ì¬í•˜ëŠ” ë‹¨ì–´ì´ì, ëª…ì‚¬ì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
        "            \"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: chat_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")"
      ],
      "metadata": {
        "id": "dQoRukC8hx6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_messages(chain_input):\n",
        "    stored_messages = chat_history.messages\n",
        "    if len(stored_messages) == 0:\n",
        "        return False\n",
        "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"placeholder\", \"{chat_history}\"),\n",
        "            (\n",
        "                \"user\",\n",
        "                \"ìœ„ ì±„íŒ… ë©”ì‹œì§€ëŠ” ëë§ì‡ê¸° ê²Œì„ì„ ì§„í–‰í•œ ëŒ€í™”ë‚´ìš©ì…ë‹ˆë‹¤. ì–¸ê¸‰í•œ ë‹¨ì–´ë“¤ë§Œ ë‚˜ì—´í•˜ì—¬ ì €ì¥í•´ì£¼ì„¸ìš”.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    summarization_chain = summarization_prompt | chat\n",
        "\n",
        "    # chat_history ì— ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ì„ ìš”ì•½í”„ë¡¬í”„íŠ¸ì— ì…ë ¥ & ê²°ê³¼ ì €ì¥\n",
        "    summary_message = summarization_chain.invoke({\"chat_history\": stored_messages})\n",
        "\n",
        "    # chat_history ì— ì €ì¥ë˜ì–´ìˆë˜ ê¸°ë¡ ì§€ìš°ê¸°\n",
        "    chat_history.clear()\n",
        "\n",
        "    # ìƒì„±ëœ ìƒˆë¡œìš´ ìš”ì•½ë‚´ìš©ìœ¼ë¡œ ê¸°ë¡ ì±„ìš°ê¸°\n",
        "    chat_history.add_message(summary_message)\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "chain_with_summarization = (\n",
        "    RunnablePassthrough.assign(messages_summarized=summarize_messages)\n",
        "    | chain_with_message_history\n",
        ")"
      ],
      "metadata": {
        "id": "BLMCkKMphx6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "    user_input = input(\"ğŸ™‹â€â™‚ï¸ YOUR TURN : \")\n",
        "    if user_input == \"ì¢…ë£Œ\": break\n",
        "    response = chain_with_summarization.invoke(\n",
        "                {\"input\": user_input},\n",
        "                {\"configurable\": {\"session_id\": \"unused\"}},\n",
        "            )\n",
        "    print(\"âœ AI TURN : \", response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDn2jJJGiw1a",
        "outputId": "4e539b43-a597-44dd-d41e-e5b959614c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ™‹â€â™‚ï¸ YOUR TURN : ë°”ë‚˜ë‚˜\n",
            "âœ AI TURN :  ë‚˜ë¬´\n",
            "ğŸ™‹â€â™‚ï¸ YOUR TURN : ë¬´ì§€ê°œ\n",
            "âœ AI TURN :  ê°œêµ¬ë¦¬\n",
            "ğŸ™‹â€â™‚ï¸ YOUR TURN : ì´ë¹¨\n",
            "âœ AI TURN :  ë¹¨ë˜\n",
            "ğŸ™‹â€â™‚ï¸ YOUR TURN : ë˜ë¯¸ì½˜\n",
            "âœ AI TURN :  ì½˜ì„œíŠ¸\n",
            "ğŸ™‹â€â™‚ï¸ YOUR TURN : íŠ¸ëŸ­\n",
            "âœ AI TURN :  ëŸ­ë¹„\n",
            "ğŸ™‹â€â™‚ï¸ YOUR TURN : ë¹„í–‰ê¸°\n",
            "âœ AI TURN :  ê¸°ì°¨\n",
            "ğŸ™‹â€â™‚ï¸ YOUR TURN : ì°¨í‘œ\n",
            "âœ AI TURN :  í‘œë²”\n",
            "ğŸ™‹â€â™‚ï¸ YOUR TURN : ë²”ê³ ë˜\n",
            "âœ AI TURN :  ë˜í”„íŒ…\n",
            "ğŸ™‹â€â™‚ï¸ YOUR TURN : íŒ…íŒ…íƒ±íƒ±\n",
            "âœ AI TURN :  ì£„ì†¡í•˜ì§€ë§Œ \"íŒ…íŒ…íƒ±íƒ±\"ì€ êµ­ì–´ì‚¬ì „ì— ì¡´ì¬í•˜ëŠ” ëª…ì‚¬ê°€ ì•„ë‹™ë‹ˆë‹¤. ë”°ë¼ì„œ ê·œì¹™ì„ ì–´ê¸°ì…¨ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ íŒ¨ë°°ë¡œ ëë§ì‡ê¸° ê²Œì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n",
            "ğŸ™‹â€â™‚ï¸ YOUR TURN : ì¢…ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "feZwCT4zsEWh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}